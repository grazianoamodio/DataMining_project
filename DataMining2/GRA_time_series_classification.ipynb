{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DGXym5D6MIZ9"
      },
      "source": [
        "# classification\n",
        "In questa parte andiamo a svolgere le task di classification del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0VqDgBp5MIaB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ygJvaAjyMIaC",
        "outputId": "9097b9f5-fef4-49c2-add9-fcac11c09840"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1828, 38038) (624, 38038)\n"
          ]
        }
      ],
      "source": [
        "X_train_dec = np.load(\"X_train_decimato.npy\")\n",
        "X_test_dec = np.load(\"X_test_decimato.npy\")\n",
        "print(X_train_dec.shape, X_test_dec.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qcVsxEKXMIaE"
      },
      "outputs": [],
      "source": [
        "y_train_df = pd.read_csv('RavdessAudioOnlyRavdessAudioOnlyNumpy__Y_train.csv')\n",
        "y_test_df = pd.read_csv('RavdessAudioOnlyRavdessAudioOnlyNumpy__Y_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r7lfo30RMIaE"
      },
      "outputs": [],
      "source": [
        "y_train = y_train_df[['vocal_channel']]\n",
        "y_test = y_test_df[['vocal_channel']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "efUt_dh4MIaF",
        "outputId": "39344089-32ad-4739-ee1f-cf3c82d0c787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1828, 7500)\n",
            "(624, 7500)\n"
          ]
        }
      ],
      "source": [
        "# can i try to cut the first seconds of the time series and latest seconds\n",
        "X_train_cut = X_train_dec[:, 7500:15000]\n",
        "X_test_cut = X_test_dec[:, 7500:15000]\n",
        "print(X_train_cut.shape)\n",
        "print(X_test_cut.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KlrN_55LMIaF"
      },
      "outputs": [],
      "source": [
        "X_train = X_train_cut\n",
        "X_test = X_test_cut"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2Urd961QMRJR"
      },
      "source": [
        "# parte per Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czemCac6MWJf",
        "outputId": "d6f41583-43fb-4885-dedb-9875beedad5c"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "573d2cd1"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA4PY8kWMWC9",
        "outputId": "c5967ff3-b308-4236-fd61-93eb039b235b"
      },
      "outputs": [],
      "source": [
        "#X_train_dec = np.load(\"/content/drive/MyDrive/Colab Notebooks/X_train_decimato.npy\")\n",
        "#X_test_dec = np.load(\"/content/drive/MyDrive/Colab Notebooks/X_test_decimato.npy\")\n",
        "#print(X_train_dec.shape, X_test_dec.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EWOjrmp6MV56"
      },
      "outputs": [],
      "source": [
        "#y_train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/RavdessAudioOnlyRavdessAudioOnlyNumpy__Y_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1Z_AQRusM1oR"
      },
      "outputs": [],
      "source": [
        "#y_test_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/RavdessAudioOnlyRavdessAudioOnlyNumpy__Y_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aG8wxW-VM1fF"
      },
      "outputs": [],
      "source": [
        "y_train1 = y_train_df[['vocal_channel']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "82UE3r_pM7cS"
      },
      "outputs": [],
      "source": [
        "y_test1 = y_test_df[['vocal_channel']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH0ai4lJNB2n",
        "outputId": "116af75a-ac99-4c4d-9b06-4ba940bacc4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1828, 7500)\n",
            "(624, 7500)\n"
          ]
        }
      ],
      "source": [
        "# can i try to cut the first seconds of the time series and latest seconds\n",
        "X_train = X_train_dec[:, 7500:15000]\n",
        "X_test= X_test_dec[:, 7500:15000]\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9jAvN_zSCAK",
        "outputId": "ec851f63-427c-4e7b-b885-1d0b682fe239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'song': 0, 'speech': 1}\n",
            "[1 1 1 ... 1 1 1]\n",
            "{'song': 0, 'speech': 1}\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Inizializza l'oggetto LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Trasforma le etichette in valori numerici (0 e 1)\n",
        "y_train = label_encoder.fit_transform(y_train1)\n",
        "\n",
        "# Verifica i mapping delle classi\n",
        "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(class_mapping)  # Questo mostrerà il mapping delle classi\n",
        "\n",
        "# Ora y_train_encoded conterrà i valori 0 e 1\n",
        "print(y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Inizializza l'oggetto LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Trasforma le etichette in valori numerici (0 e 1)\n",
        "y_test = label_encoder.fit_transform(y_test1)\n",
        "\n",
        "# Verifica i mapping delle classi\n",
        "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(class_mapping)  # Questo mostrerà il mapping delle classi\n",
        "\n",
        "# Ora y_train_encoded conterrà i valori 0 e 1\n",
        "print(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ_HgNb1S3Nj",
        "outputId": "257ca5e8-c989-4cda-d900-d22694195589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numero di 0: 748\n",
            "Numero di 1: 1080\n",
            "Numero di 0: 264\n",
            "Numero di 1: 360\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Conta il numero di 1 nell'array y_train_encoded\n",
        "num_ones = np.count_nonzero(y_train)\n",
        "\n",
        "# Calcola il numero di 0 sottraendo il numero di 1 dalla lunghezza dell'array\n",
        "num_zeros = len(y_train) - num_ones\n",
        "\n",
        "# Stampa i risultati\n",
        "print(\"Numero di 0:\", num_zeros)\n",
        "print(\"Numero di 1:\", num_ones)\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Conta il numero di 1 nell'array y_train_encoded\n",
        "num_ones = np.count_nonzero(y_test)\n",
        "\n",
        "# Calcola il numero di 0 sottraendo il numero di 1 dalla lunghezza dell'array\n",
        "num_zeros = len(y_test) - num_ones\n",
        "\n",
        "# Stampa i risultati\n",
        "print(\"Numero di 0:\", num_zeros)\n",
        "print(\"Numero di 1:\", num_ones)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9K2isYwBMIaH"
      },
      "source": [
        "# time series classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAOve3kyNe24",
        "outputId": "3732bb45-2d12-4633-d838-0f990c4be82f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kds in /opt/anaconda3/lib/python3.9/site-packages (0.1.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /opt/anaconda3/lib/python3.9/site-packages (from kds) (1.21.5)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /opt/anaconda3/lib/python3.9/site-packages (from kds) (1.4.4)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from kds) (3.5.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->kds) (9.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->kds) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->kds) (21.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->kds) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->kds) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->kds) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->kds) (1.4.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.20.0->kds) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->kds) (1.16.0)\n",
            "Requirement already satisfied: pyts in /opt/anaconda3/lib/python3.9/site-packages (0.12.0)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from pyts) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /opt/anaconda3/lib/python3.9/site-packages (from pyts) (1.2.0)\n",
            "Requirement already satisfied: joblib>=0.12 in /opt/anaconda3/lib/python3.9/site-packages (from pyts) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.5 in /opt/anaconda3/lib/python3.9/site-packages (from pyts) (1.21.5)\n",
            "Requirement already satisfied: numba>=0.48.0 in /opt/anaconda3/lib/python3.9/site-packages (from pyts) (0.55.1)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from numba>=0.48.0->pyts) (63.4.1)\n",
            "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/anaconda3/lib/python3.9/site-packages (from numba>=0.48.0->pyts) (0.38.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.22.1->pyts) (2.2.0)\n",
            "Requirement already satisfied: tslearn in /opt/anaconda3/lib/python3.9/site-packages (0.5.3.2)\n",
            "Requirement already satisfied: numba in /opt/anaconda3/lib/python3.9/site-packages (from tslearn) (0.55.1)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.9/site-packages (from tslearn) (1.7.3)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.9/site-packages (from tslearn) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.9/site-packages (from tslearn) (1.2.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.9/site-packages (from tslearn) (1.21.5)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from numba->tslearn) (63.4.1)\n",
            "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/anaconda3/lib/python3.9/site-packages (from numba->tslearn) (0.38.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->tslearn) (2.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install kds\n",
        "!pip3 install pyts\n",
        "!pip3 install tslearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hbg_YD1PMIaI"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "#Main classification metrics and utilities\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import kds #for the lift\n",
        "\n",
        "#Classifiers\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from pyts.classification import KNeighborsClassifier\n",
        "from tslearn.neighbors import KNeighborsTimeSeriesClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ASDLLTpMIaI",
        "outputId": "04a7a3e2-d822-4531-f992-6fb83266cc0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.7003205128205128\n",
            "F1-score [0.46418338 0.7919911 ]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.31      0.46       264\n",
            "           1       0.66      0.99      0.79       360\n",
            "\n",
            "    accuracy                           0.70       624\n",
            "   macro avg       0.81      0.65      0.63       624\n",
            "weighted avg       0.78      0.70      0.65       624\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = KNeighborsClassifier(n_neighbors=3, weights='uniform')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "clOtWVKXMIaJ"
      },
      "source": [
        "adesso provo velocemente a vedere cosa cambia se normalizzo i dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "_Zmuck_8MIaJ"
      },
      "outputs": [],
      "source": [
        "# normalizzazione dei dati\n",
        "from tslearn.preprocessing import TimeSeriesScalerMinMax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "kZWw9jLBMIaK"
      },
      "outputs": [],
      "source": [
        "scaler = TimeSeriesScalerMinMax()\n",
        "X_train_s = scaler.fit_transform(X_train).reshape(X_train.shape[0], X_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ibHpVeWrMIaK"
      },
      "outputs": [],
      "source": [
        "scaler = TimeSeriesScalerMinMax()\n",
        "X_test_s = scaler.fit_transform(X_test).reshape(X_test.shape[0], X_test.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "UcfnonBiMIaK",
        "outputId": "b853470f-ae9a-4cda-e94f-91a93ffac679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.5769230769230769\n",
            "F1-score [0.         0.73170732]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       264\n",
            "           1       0.58      1.00      0.73       360\n",
            "\n",
            "    accuracy                           0.58       624\n",
            "   macro avg       0.29      0.50      0.37       624\n",
            "weighted avg       0.33      0.58      0.42       624\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "clf = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
        "clf.fit(X_train_s, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e-GlBUqCMIaL"
      },
      "source": [
        "come è possibile notare i dati non vanno normalizzati."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8Z8piQigMIaM"
      },
      "outputs": [],
      "source": [
        "# CNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "osam8A0kMIaM"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling1D\n",
        "from keras.layers import Conv1D, Activation, Conv1D, BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Rjy7YWu2MIaN"
      },
      "outputs": [],
      "source": [
        "def build_simple_cnn(n_timesteps, n_outputs):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv1D(filters=16, kernel_size=8, activation='relu', input_shape=(n_timesteps, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "\n",
        "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib3KGdEPMIaO",
        "outputId": "597b1f85-f71f-4844-ef79-255b9a11043d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TIMESTEPS:  7500\n",
            "N. LABELS:  2\n"
          ]
        }
      ],
      "source": [
        "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(X_train_cnn, y_train, test_size=0.2, stratify=y_train)\n",
        "\n",
        "n_timesteps, n_outputs, n_features = X_train_cnn.shape[1], len(np.unique(y_train_cnn)), 1\n",
        "print(\"TIMESTEPS: \", n_timesteps)\n",
        "print(\"N. LABELS: \", n_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "y47fbxFvMIaO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-07 12:34:25.754289: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "cnn = build_simple_cnn(n_timesteps, n_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPDMOOE9MIaP",
        "outputId": "26f04d94-cc92-475b-cd44-22c983104545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 7493, 16)          144       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 7493, 16)         64        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 7493, 16)          0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 7493, 16)          0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 7489, 32)          2592      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 7489, 32)         128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 7489, 32)          0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 7489, 32)          0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 7487, 64)          6208      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 7487, 64)         256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 7487, 64)          0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 7487, 64)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 64)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,522\n",
            "Trainable params: 9,298\n",
            "Non-trainable params: 224\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DQijXOPKMIaP"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8HCbCyGOMIaQ"
      },
      "outputs": [],
      "source": [
        "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
        "mc = ModelCheckpoint('best_model_cnn.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "callbacks = [rlr, mc]\n",
        "\n",
        "batch_size = 16\n",
        "mini_batch_size = int(min(X_train_cnn.shape[0]/10, batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0BpQKBiMIaQ",
        "outputId": "8e3bb92d-008f-4ee9-ca99-843a8f159fc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "92/92 [==============================] - 107s 1s/step - loss: 0.6561 - accuracy: 0.6334 - val_loss: 0.6844 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 2/5\n",
            "92/92 [==============================] - 78s 847ms/step - loss: 0.5584 - accuracy: 0.7038 - val_loss: 0.6830 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 3/5\n",
            "92/92 [==============================] - 87s 942ms/step - loss: 0.5444 - accuracy: 0.7230 - val_loss: 0.6861 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 4/5\n",
            "92/92 [==============================] - 80s 868ms/step - loss: 0.5250 - accuracy: 0.7401 - val_loss: 0.6833 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 5/5\n",
            "92/92 [==============================] - 66s 723ms/step - loss: 0.5035 - accuracy: 0.7572 - val_loss: 0.6901 - val_accuracy: 0.4563 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "history_cnn = cnn.fit(X_train_cnn, y_train_cnn, epochs=5, batch_size=mini_batch_size, callbacks=callbacks,\n",
        "                      validation_data=(X_val_cnn, y_val_cnn)).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 4s 194ms/step\n",
            "Accuracy 0.46955128205128205\n",
            "F1-score [0.60920897 0.17456359]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.98      0.61       264\n",
            "           1       0.85      0.10      0.17       360\n",
            "\n",
            "    accuracy                           0.47       624\n",
            "   macro avg       0.65      0.54      0.39       624\n",
            "weighted avg       0.68      0.47      0.36       624\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = np.argmax(cnn.predict(X_test_cnn), axis=1)\n",
        "\n",
        "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 3s 164ms/step - loss: 0.6869 - accuracy: 0.4696\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.6869125366210938, 0.46955129504203796]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnn.evaluate(X_test_cnn, y_test)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KNN con la DTW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastdtw in /opt/anaconda3/lib/python3.9/site-packages (0.3.4)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.9/site-packages (from fastdtw) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastdtw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastdtw import fastdtw\n",
        "from tslearn.metrics import dtw_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastdtw import fastdtw\n",
        "\n",
        "def dtw_itakura(x, y):\n",
        "    distance, _ = fastdtw(x, y)\n",
        "    return distance\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5, metric=dtw_itakura)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
        "scaler = TimeSeriesScalerMinMax()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_reduced = np.mean(X_train_scaled, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9022435897435898\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from tslearn.metrics import dtw\n",
        "\n",
        "\n",
        "\n",
        "# Inizializzazione del classificatore KNN\n",
        "k = 5  # Numero di vicini da considerare\n",
        "knn = KNeighborsClassifier(n_neighbors=5, metric=dtw_itakura)\n",
        "\n",
        "# Addestramento del modello KNN\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Classificazione delle serie temporali di test\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Valutazione delle prestazioni del modello\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.9022435897435898\n",
            "F1-score [0.89009009 0.91197691]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.94      0.89       264\n",
            "           1       0.95      0.88      0.91       360\n",
            "\n",
            "    accuracy                           0.90       624\n",
            "   macro avg       0.90      0.91      0.90       624\n",
            "weighted avg       0.91      0.90      0.90       624\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### distanza euclidea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6410256410256411\n",
            "Accuracy 0.6410256410256411\n",
            "F1-score [0.27272727 0.76170213]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.16      0.27       264\n",
            "           1       0.62      0.99      0.76       360\n",
            "\n",
            "    accuracy                           0.64       624\n",
            "   macro avg       0.79      0.58      0.52       624\n",
            "weighted avg       0.76      0.64      0.55       624\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# adesso provo con la euclidean \n",
        "# Inizializzazione del classificatore KNN\n",
        "k = 5  # Numero di vicini da considerare\n",
        "knn_eu = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "\n",
        "# Addestramento del modello KNN\n",
        "knn_eu.fit(X_train, y_train)\n",
        "\n",
        "# Classificazione delle serie temporali di test\n",
        "y_pred = knn_eu.predict(X_test)\n",
        "\n",
        "# Valutazione delle prestazioni del modello\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#LSTM prova "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.layers import LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def build_lstm(n_timesteps, n_outputs):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(256, input_shape=(n_timesteps, 1)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
        "    \n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "lstm = build_lstm(n_timesteps, n_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 256)               264192    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 280,770\n",
            "Trainable params: 280,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "92/92 [==============================] - 82s 842ms/step - loss: 0.6794 - accuracy: 0.5780 - val_loss: 0.6835 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 2/300\n",
            "92/92 [==============================] - 70s 762ms/step - loss: 0.6396 - accuracy: 0.6847 - val_loss: 0.6746 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 3/300\n",
            "92/92 [==============================] - 59s 643ms/step - loss: 0.5601 - accuracy: 0.7332 - val_loss: 0.6871 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 4/300\n",
            "92/92 [==============================] - 59s 637ms/step - loss: 0.5213 - accuracy: 0.7538 - val_loss: 0.6794 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 5/300\n",
            "92/92 [==============================] - 63s 686ms/step - loss: 0.5000 - accuracy: 0.7510 - val_loss: 0.9587 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 6/300\n",
            "92/92 [==============================] - 55s 602ms/step - loss: 0.4861 - accuracy: 0.7715 - val_loss: 2.6165 - val_accuracy: 0.4180 - lr: 0.0010\n",
            "Epoch 7/300\n",
            "92/92 [==============================] - 53s 571ms/step - loss: 0.4659 - accuracy: 0.7818 - val_loss: 0.7301 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 8/300\n",
            "92/92 [==============================] - 52s 569ms/step - loss: 0.4490 - accuracy: 0.7969 - val_loss: 1.1500 - val_accuracy: 0.4454 - lr: 0.0010\n",
            "Epoch 9/300\n",
            "92/92 [==============================] - 52s 569ms/step - loss: 0.4500 - accuracy: 0.7866 - val_loss: 1.9264 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 10/300\n",
            "92/92 [==============================] - 52s 566ms/step - loss: 0.4348 - accuracy: 0.8126 - val_loss: 12.4508 - val_accuracy: 0.4153 - lr: 0.0010\n",
            "Epoch 11/300\n",
            "92/92 [==============================] - 56s 605ms/step - loss: 0.4348 - accuracy: 0.8016 - val_loss: 1.0606 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 12/300\n",
            "92/92 [==============================] - 55s 594ms/step - loss: 0.4147 - accuracy: 0.8215 - val_loss: 2.0959 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 13/300\n",
            "92/92 [==============================] - 52s 563ms/step - loss: 0.4138 - accuracy: 0.8174 - val_loss: 6.8932 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 14/300\n",
            "92/92 [==============================] - 65s 710ms/step - loss: 0.3996 - accuracy: 0.8290 - val_loss: 0.6946 - val_accuracy: 0.6667 - lr: 0.0010\n",
            "Epoch 15/300\n",
            "92/92 [==============================] - 61s 664ms/step - loss: 0.3856 - accuracy: 0.8393 - val_loss: 11.2204 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 16/300\n",
            "92/92 [==============================] - 64s 700ms/step - loss: 0.3896 - accuracy: 0.8379 - val_loss: 1.2012 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 17/300\n",
            "92/92 [==============================] - 61s 659ms/step - loss: 0.3619 - accuracy: 0.8482 - val_loss: 6.9005 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 18/300\n",
            "92/92 [==============================] - 56s 611ms/step - loss: 0.3654 - accuracy: 0.8475 - val_loss: 6.1319 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 19/300\n",
            "92/92 [==============================] - 52s 566ms/step - loss: 0.3878 - accuracy: 0.8365 - val_loss: 0.9002 - val_accuracy: 0.6530 - lr: 0.0010\n",
            "Epoch 20/300\n",
            "92/92 [==============================] - 52s 561ms/step - loss: 0.3702 - accuracy: 0.8570 - val_loss: 2.5566 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 21/300\n",
            "92/92 [==============================] - 52s 567ms/step - loss: 0.3533 - accuracy: 0.8543 - val_loss: 0.8481 - val_accuracy: 0.6393 - lr: 0.0010\n",
            "Epoch 22/300\n",
            "92/92 [==============================] - 58s 632ms/step - loss: 0.3578 - accuracy: 0.8461 - val_loss: 1.3535 - val_accuracy: 0.5929 - lr: 0.0010\n",
            "Epoch 23/300\n",
            "92/92 [==============================] - 52s 566ms/step - loss: 0.3435 - accuracy: 0.8591 - val_loss: 0.8511 - val_accuracy: 0.6120 - lr: 0.0010\n",
            "Epoch 24/300\n",
            "92/92 [==============================] - 52s 564ms/step - loss: 0.3548 - accuracy: 0.8393 - val_loss: 2.2614 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 25/300\n",
            "92/92 [==============================] - 52s 566ms/step - loss: 0.3383 - accuracy: 0.8659 - val_loss: 9.7330 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 26/300\n",
            "92/92 [==============================] - 52s 566ms/step - loss: 0.3400 - accuracy: 0.8687 - val_loss: 9.0623 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 27/300\n",
            "92/92 [==============================] - 52s 571ms/step - loss: 0.3246 - accuracy: 0.8659 - val_loss: 3.6561 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 28/300\n",
            "92/92 [==============================] - 52s 566ms/step - loss: 0.3220 - accuracy: 0.8707 - val_loss: 1.5207 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 29/300\n",
            "92/92 [==============================] - 59s 647ms/step - loss: 0.3127 - accuracy: 0.8803 - val_loss: 2.5754 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 30/300\n",
            "92/92 [==============================] - 60s 651ms/step - loss: 0.3098 - accuracy: 0.8735 - val_loss: 3.6040 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 31/300\n",
            "92/92 [==============================] - 54s 591ms/step - loss: 0.3196 - accuracy: 0.8653 - val_loss: 2.6323 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 32/300\n",
            "92/92 [==============================] - 71s 778ms/step - loss: 0.3049 - accuracy: 0.8728 - val_loss: 4.0460 - val_accuracy: 0.4344 - lr: 0.0010\n",
            "Epoch 33/300\n",
            "92/92 [==============================] - 91s 987ms/step - loss: 0.2883 - accuracy: 0.8865 - val_loss: 1.5359 - val_accuracy: 0.6393 - lr: 0.0010\n",
            "Epoch 34/300\n",
            "92/92 [==============================] - 82s 886ms/step - loss: 0.2984 - accuracy: 0.8824 - val_loss: 6.5696 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 35/300\n",
            "92/92 [==============================] - 78s 852ms/step - loss: 0.2747 - accuracy: 0.8988 - val_loss: 3.4310 - val_accuracy: 0.4235 - lr: 0.0010\n",
            "Epoch 36/300\n",
            "92/92 [==============================] - 78s 842ms/step - loss: 0.2859 - accuracy: 0.8865 - val_loss: 8.0641 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 37/300\n",
            "92/92 [==============================] - 94s 1s/step - loss: 0.2783 - accuracy: 0.8878 - val_loss: 1.4374 - val_accuracy: 0.5683 - lr: 0.0010\n",
            "Epoch 38/300\n",
            "92/92 [==============================] - 60s 656ms/step - loss: 0.2757 - accuracy: 0.8885 - val_loss: 8.8247 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 39/300\n",
            "92/92 [==============================] - 73s 798ms/step - loss: 0.2799 - accuracy: 0.8919 - val_loss: 5.2482 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 40/300\n",
            "92/92 [==============================] - 82s 890ms/step - loss: 0.3018 - accuracy: 0.8762 - val_loss: 6.5184 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 41/300\n",
            "92/92 [==============================] - 72s 782ms/step - loss: 0.2703 - accuracy: 0.8906 - val_loss: 4.7418 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 42/300\n",
            "92/92 [==============================] - 66s 717ms/step - loss: 0.2650 - accuracy: 0.8974 - val_loss: 9.8087 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 43/300\n",
            "92/92 [==============================] - 64s 692ms/step - loss: 0.2703 - accuracy: 0.8960 - val_loss: 7.1782 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 44/300\n",
            "92/92 [==============================] - 65s 706ms/step - loss: 0.2885 - accuracy: 0.8810 - val_loss: 6.9233 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 45/300\n",
            "92/92 [==============================] - 68s 741ms/step - loss: 0.2509 - accuracy: 0.9036 - val_loss: 6.9385 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 46/300\n",
            "92/92 [==============================] - 65s 703ms/step - loss: 0.2753 - accuracy: 0.8953 - val_loss: 3.9317 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 47/300\n",
            "92/92 [==============================] - 55s 593ms/step - loss: 0.2657 - accuracy: 0.8974 - val_loss: 4.1460 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 48/300\n",
            "92/92 [==============================] - 54s 591ms/step - loss: 0.2545 - accuracy: 0.8960 - val_loss: 4.4015 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 49/300\n",
            "92/92 [==============================] - 57s 614ms/step - loss: 0.2543 - accuracy: 0.9070 - val_loss: 2.6728 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 50/300\n",
            "92/92 [==============================] - 58s 636ms/step - loss: 0.2554 - accuracy: 0.9001 - val_loss: 3.7393 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 51/300\n",
            "92/92 [==============================] - 54s 586ms/step - loss: 0.2566 - accuracy: 0.9022 - val_loss: 6.4065 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 52/300\n",
            "92/92 [==============================] - 54s 587ms/step - loss: 0.2493 - accuracy: 0.8995 - val_loss: 5.4513 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 53/300\n",
            "92/92 [==============================] - 54s 587ms/step - loss: 0.2447 - accuracy: 0.9049 - val_loss: 13.1812 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 54/300\n",
            "92/92 [==============================] - 54s 584ms/step - loss: 0.2612 - accuracy: 0.8995 - val_loss: 2.1139 - val_accuracy: 0.5000 - lr: 0.0010\n",
            "Epoch 55/300\n",
            "92/92 [==============================] - 55s 595ms/step - loss: 0.2557 - accuracy: 0.9029 - val_loss: 1.9830 - val_accuracy: 0.6011 - lr: 0.0010\n",
            "Epoch 56/300\n",
            "92/92 [==============================] - 53s 580ms/step - loss: 0.2390 - accuracy: 0.9077 - val_loss: 10.4577 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 57/300\n",
            "92/92 [==============================] - 54s 591ms/step - loss: 0.2356 - accuracy: 0.9111 - val_loss: 8.1405 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 58/300\n",
            "92/92 [==============================] - 54s 584ms/step - loss: 0.2470 - accuracy: 0.9070 - val_loss: 5.3926 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 59/300\n",
            "92/92 [==============================] - 54s 586ms/step - loss: 0.2584 - accuracy: 0.9049 - val_loss: 4.5125 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 60/300\n",
            "92/92 [==============================] - 56s 610ms/step - loss: 0.2483 - accuracy: 0.9022 - val_loss: 3.9780 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 61/300\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.2374 - accuracy: 0.9083 - val_loss: 5.1139 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 62/300\n",
            "92/92 [==============================] - 54s 590ms/step - loss: 0.2454 - accuracy: 0.8981 - val_loss: 4.3721 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 63/300\n",
            "92/92 [==============================] - 54s 587ms/step - loss: 0.2440 - accuracy: 0.9077 - val_loss: 10.5495 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 64/300\n",
            "92/92 [==============================] - 55s 593ms/step - loss: 0.2155 - accuracy: 0.9159 - val_loss: 3.1360 - val_accuracy: 0.5628 - lr: 0.0010\n",
            "Epoch 65/300\n",
            "92/92 [==============================] - 54s 590ms/step - loss: 0.2574 - accuracy: 0.9056 - val_loss: 11.0345 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 66/300\n",
            "92/92 [==============================] - 55s 591ms/step - loss: 0.2355 - accuracy: 0.9104 - val_loss: 1.7024 - val_accuracy: 0.6066 - lr: 0.0010\n",
            "Epoch 67/300\n",
            "92/92 [==============================] - 54s 589ms/step - loss: 0.2320 - accuracy: 0.9104 - val_loss: 4.9738 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 68/300\n",
            "92/92 [==============================] - 54s 587ms/step - loss: 0.2142 - accuracy: 0.9220 - val_loss: 6.9021 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 69/300\n",
            "92/92 [==============================] - 54s 590ms/step - loss: 0.2386 - accuracy: 0.8988 - val_loss: 5.1127 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 70/300\n",
            "92/92 [==============================] - 54s 589ms/step - loss: 0.2176 - accuracy: 0.9152 - val_loss: 6.6868 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 71/300\n",
            "92/92 [==============================] - 57s 620ms/step - loss: 0.2071 - accuracy: 0.9200 - val_loss: 5.4597 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 72/300\n",
            "92/92 [==============================] - 58s 631ms/step - loss: 0.2176 - accuracy: 0.9207 - val_loss: 3.8240 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 73/300\n",
            "92/92 [==============================] - 54s 588ms/step - loss: 0.2140 - accuracy: 0.9200 - val_loss: 7.0663 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 74/300\n",
            "92/92 [==============================] - 54s 588ms/step - loss: 0.2229 - accuracy: 0.9104 - val_loss: 3.0766 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 75/300\n",
            "92/92 [==============================] - 54s 591ms/step - loss: 0.2168 - accuracy: 0.9166 - val_loss: 8.8925 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 76/300\n",
            "92/92 [==============================] - 55s 598ms/step - loss: 0.2312 - accuracy: 0.9118 - val_loss: 1.8662 - val_accuracy: 0.6066 - lr: 0.0010\n",
            "Epoch 77/300\n",
            "92/92 [==============================] - 55s 595ms/step - loss: 0.2278 - accuracy: 0.9152 - val_loss: 5.9152 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 78/300\n",
            "92/92 [==============================] - 54s 588ms/step - loss: 0.2180 - accuracy: 0.9179 - val_loss: 11.9136 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 79/300\n",
            "92/92 [==============================] - 54s 590ms/step - loss: 0.2138 - accuracy: 0.9179 - val_loss: 3.4537 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 80/300\n",
            "92/92 [==============================] - 55s 595ms/step - loss: 0.2373 - accuracy: 0.9042 - val_loss: 10.9264 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 81/300\n",
            "92/92 [==============================] - 54s 589ms/step - loss: 0.2117 - accuracy: 0.9248 - val_loss: 7.1659 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 82/300\n",
            "92/92 [==============================] - 57s 621ms/step - loss: 0.2496 - accuracy: 0.9022 - val_loss: 5.4924 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 83/300\n",
            "92/92 [==============================] - 59s 636ms/step - loss: 0.2107 - accuracy: 0.9152 - val_loss: 6.9859 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 84/300\n",
            "92/92 [==============================] - 55s 595ms/step - loss: 0.1972 - accuracy: 0.9282 - val_loss: 10.7595 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 85/300\n",
            "92/92 [==============================] - 54s 592ms/step - loss: 0.2001 - accuracy: 0.9289 - val_loss: 5.1881 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 86/300\n",
            "92/92 [==============================] - 54s 592ms/step - loss: 0.2105 - accuracy: 0.9193 - val_loss: 5.9299 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 87/300\n",
            "92/92 [==============================] - 55s 593ms/step - loss: 0.2079 - accuracy: 0.9234 - val_loss: 7.8928 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 88/300\n",
            "92/92 [==============================] - 55s 597ms/step - loss: 0.2288 - accuracy: 0.9172 - val_loss: 11.2193 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 89/300\n",
            "92/92 [==============================] - 54s 591ms/step - loss: 0.1945 - accuracy: 0.9289 - val_loss: 6.1659 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 90/300\n",
            "92/92 [==============================] - 55s 594ms/step - loss: 0.2059 - accuracy: 0.9268 - val_loss: 3.1863 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 91/300\n",
            "92/92 [==============================] - 54s 589ms/step - loss: 0.2005 - accuracy: 0.9254 - val_loss: 3.4135 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 92/300\n",
            "92/92 [==============================] - 55s 596ms/step - loss: 0.2036 - accuracy: 0.9282 - val_loss: 4.5975 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 93/300\n",
            "92/92 [==============================] - 60s 648ms/step - loss: 0.1947 - accuracy: 0.9289 - val_loss: 7.8301 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 94/300\n",
            "92/92 [==============================] - 58s 629ms/step - loss: 0.1992 - accuracy: 0.9179 - val_loss: 4.1634 - val_accuracy: 0.5137 - lr: 0.0010\n",
            "Epoch 95/300\n",
            "92/92 [==============================] - 54s 592ms/step - loss: 0.2081 - accuracy: 0.9282 - val_loss: 3.4875 - val_accuracy: 0.5574 - lr: 0.0010\n",
            "Epoch 96/300\n",
            "92/92 [==============================] - 55s 593ms/step - loss: 0.1922 - accuracy: 0.9268 - val_loss: 2.6281 - val_accuracy: 0.5656 - lr: 0.0010\n",
            "Epoch 97/300\n",
            "92/92 [==============================] - 54s 592ms/step - loss: 0.1991 - accuracy: 0.9227 - val_loss: 3.3795 - val_accuracy: 0.5738 - lr: 0.0010\n",
            "Epoch 98/300\n",
            "92/92 [==============================] - 56s 612ms/step - loss: 0.2115 - accuracy: 0.9227 - val_loss: 2.5893 - val_accuracy: 0.6120 - lr: 0.0010\n",
            "Epoch 99/300\n",
            "92/92 [==============================] - 55s 598ms/step - loss: 0.1989 - accuracy: 0.9172 - val_loss: 15.8011 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 100/300\n",
            "92/92 [==============================] - 54s 590ms/step - loss: 0.1951 - accuracy: 0.9289 - val_loss: 6.7267 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 101/300\n",
            "92/92 [==============================] - 55s 591ms/step - loss: 0.2051 - accuracy: 0.9207 - val_loss: 0.7490 - val_accuracy: 0.7514 - lr: 0.0010\n",
            "Epoch 102/300\n",
            "92/92 [==============================] - 55s 596ms/step - loss: 0.1982 - accuracy: 0.9282 - val_loss: 8.4661 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 103/300\n",
            "92/92 [==============================] - 55s 592ms/step - loss: 0.1884 - accuracy: 0.9316 - val_loss: 8.4503 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 104/300\n",
            "92/92 [==============================] - 60s 652ms/step - loss: 0.2017 - accuracy: 0.9207 - val_loss: 2.0285 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 105/300\n",
            "92/92 [==============================] - 56s 603ms/step - loss: 0.1875 - accuracy: 0.9309 - val_loss: 10.8082 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 106/300\n",
            "92/92 [==============================] - 55s 596ms/step - loss: 0.1744 - accuracy: 0.9364 - val_loss: 11.9942 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 107/300\n",
            "92/92 [==============================] - 55s 594ms/step - loss: 0.1925 - accuracy: 0.9350 - val_loss: 12.3323 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 108/300\n",
            "92/92 [==============================] - 55s 595ms/step - loss: 0.1944 - accuracy: 0.9234 - val_loss: 2.6025 - val_accuracy: 0.5628 - lr: 0.0010\n",
            "Epoch 109/300\n",
            "92/92 [==============================] - 55s 595ms/step - loss: 0.1831 - accuracy: 0.9364 - val_loss: 9.5701 - val_accuracy: 0.4317 - lr: 0.0010\n",
            "Epoch 110/300\n",
            "92/92 [==============================] - 55s 601ms/step - loss: 0.1809 - accuracy: 0.9337 - val_loss: 3.4859 - val_accuracy: 0.5109 - lr: 0.0010\n",
            "Epoch 111/300\n",
            "92/92 [==============================] - 54s 591ms/step - loss: 0.1842 - accuracy: 0.9323 - val_loss: 8.5990 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 112/300\n",
            "92/92 [==============================] - 55s 594ms/step - loss: 0.2060 - accuracy: 0.9118 - val_loss: 14.2095 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 113/300\n",
            "92/92 [==============================] - 54s 589ms/step - loss: 0.1959 - accuracy: 0.9213 - val_loss: 8.9987 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 114/300\n",
            "92/92 [==============================] - 55s 598ms/step - loss: 0.1745 - accuracy: 0.9446 - val_loss: 7.2173 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 115/300\n",
            "92/92 [==============================] - 61s 662ms/step - loss: 0.1744 - accuracy: 0.9364 - val_loss: 11.8290 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 116/300\n",
            "92/92 [==============================] - 54s 591ms/step - loss: 0.1777 - accuracy: 0.9343 - val_loss: 11.0445 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 117/300\n",
            "92/92 [==============================] - 55s 594ms/step - loss: 0.1683 - accuracy: 0.9405 - val_loss: 4.2821 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 118/300\n",
            "92/92 [==============================] - 54s 590ms/step - loss: 0.1848 - accuracy: 0.9343 - val_loss: 24.8572 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 119/300\n",
            "92/92 [==============================] - 55s 595ms/step - loss: 0.1584 - accuracy: 0.9494 - val_loss: 5.1760 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 120/300\n",
            "92/92 [==============================] - 56s 604ms/step - loss: 0.1782 - accuracy: 0.9337 - val_loss: 12.5901 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 121/300\n",
            "92/92 [==============================] - 55s 595ms/step - loss: 0.1942 - accuracy: 0.9302 - val_loss: 10.2346 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 122/300\n",
            "92/92 [==============================] - 54s 591ms/step - loss: 0.2018 - accuracy: 0.9220 - val_loss: 15.7704 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 123/300\n",
            "92/92 [==============================] - 54s 591ms/step - loss: 0.1705 - accuracy: 0.9378 - val_loss: 7.5127 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 124/300\n",
            "92/92 [==============================] - 54s 590ms/step - loss: 0.1672 - accuracy: 0.9330 - val_loss: 17.5579 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 125/300\n",
            "92/92 [==============================] - 55s 600ms/step - loss: 0.1931 - accuracy: 0.9213 - val_loss: 6.5131 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 126/300\n",
            "92/92 [==============================] - 62s 675ms/step - loss: 0.1532 - accuracy: 0.9398 - val_loss: 24.7831 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 127/300\n",
            "92/92 [==============================] - 54s 590ms/step - loss: 0.1787 - accuracy: 0.9282 - val_loss: 8.4569 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 128/300\n",
            "92/92 [==============================] - 55s 593ms/step - loss: 0.1949 - accuracy: 0.9261 - val_loss: 4.4302 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 129/300\n",
            "92/92 [==============================] - 54s 592ms/step - loss: 0.1612 - accuracy: 0.9391 - val_loss: 2.8972 - val_accuracy: 0.6148 - lr: 0.0010\n",
            "Epoch 130/300\n",
            "92/92 [==============================] - 67s 723ms/step - loss: 0.1664 - accuracy: 0.9337 - val_loss: 9.4659 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 131/300\n",
            "92/92 [==============================] - 79s 859ms/step - loss: 0.1680 - accuracy: 0.9337 - val_loss: 7.5723 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 132/300\n",
            "92/92 [==============================] - 84s 909ms/step - loss: 0.1771 - accuracy: 0.9323 - val_loss: 9.6040 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 133/300\n",
            "92/92 [==============================] - 100s 1s/step - loss: 0.1683 - accuracy: 0.9357 - val_loss: 3.8377 - val_accuracy: 0.5984 - lr: 0.0010\n",
            "Epoch 134/300\n",
            "92/92 [==============================] - 151s 2s/step - loss: 0.1696 - accuracy: 0.9378 - val_loss: 13.9987 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 135/300\n",
            "92/92 [==============================] - 115s 1s/step - loss: 0.1772 - accuracy: 0.9275 - val_loss: 2.3538 - val_accuracy: 0.6230 - lr: 0.0010\n",
            "Epoch 136/300\n",
            "92/92 [==============================] - 61s 662ms/step - loss: 0.1781 - accuracy: 0.9275 - val_loss: 9.0790 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 137/300\n",
            "92/92 [==============================] - 57s 615ms/step - loss: 0.1752 - accuracy: 0.9466 - val_loss: 6.9103 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 138/300\n",
            "92/92 [==============================] - 53s 579ms/step - loss: 0.1687 - accuracy: 0.9391 - val_loss: 0.6992 - val_accuracy: 0.7158 - lr: 0.0010\n",
            "Epoch 139/300\n",
            "92/92 [==============================] - 59s 645ms/step - loss: 0.1566 - accuracy: 0.9466 - val_loss: 1.0016 - val_accuracy: 0.6967 - lr: 0.0010\n",
            "Epoch 140/300\n",
            "92/92 [==============================] - 64s 699ms/step - loss: 0.1645 - accuracy: 0.9391 - val_loss: 16.7825 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 141/300\n",
            "92/92 [==============================] - 60s 650ms/step - loss: 0.1617 - accuracy: 0.9419 - val_loss: 5.0805 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 142/300\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.1636 - accuracy: 0.9412 - val_loss: 6.3155 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 143/300\n",
            "92/92 [==============================] - 63s 687ms/step - loss: 0.1592 - accuracy: 0.9398 - val_loss: 8.1767 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 144/300\n",
            "92/92 [==============================] - 71s 775ms/step - loss: 0.1499 - accuracy: 0.9446 - val_loss: 13.3317 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 145/300\n",
            "92/92 [==============================] - 71s 774ms/step - loss: 0.1628 - accuracy: 0.9391 - val_loss: 12.8242 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 146/300\n",
            "92/92 [==============================] - 73s 792ms/step - loss: 0.1467 - accuracy: 0.9480 - val_loss: 28.3953 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 147/300\n",
            "92/92 [==============================] - 61s 665ms/step - loss: 0.1560 - accuracy: 0.9425 - val_loss: 5.1729 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 148/300\n",
            "92/92 [==============================] - 60s 656ms/step - loss: 0.1504 - accuracy: 0.9412 - val_loss: 14.2433 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 149/300\n",
            "92/92 [==============================] - 55s 599ms/step - loss: 0.1770 - accuracy: 0.9364 - val_loss: 6.4176 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 150/300\n",
            "92/92 [==============================] - 53s 577ms/step - loss: 0.1657 - accuracy: 0.9282 - val_loss: 2.2250 - val_accuracy: 0.5956 - lr: 0.0010\n",
            "Epoch 151/300\n",
            "92/92 [==============================] - 54s 583ms/step - loss: 0.1588 - accuracy: 0.9419 - val_loss: 2.5351 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 152/300\n",
            "92/92 [==============================] - 60s 652ms/step - loss: 0.1641 - accuracy: 0.9357 - val_loss: 13.2953 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 153/300\n",
            "92/92 [==============================] - 64s 701ms/step - loss: 0.1734 - accuracy: 0.9337 - val_loss: 10.9041 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 154/300\n",
            "92/92 [==============================] - 72s 783ms/step - loss: 0.1699 - accuracy: 0.9364 - val_loss: 2.5136 - val_accuracy: 0.5191 - lr: 0.0010\n",
            "Epoch 155/300\n",
            "92/92 [==============================] - 67s 729ms/step - loss: 0.1536 - accuracy: 0.9460 - val_loss: 17.1219 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 156/300\n",
            "92/92 [==============================] - 85s 922ms/step - loss: 0.1533 - accuracy: 0.9425 - val_loss: 12.6703 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 157/300\n",
            "92/92 [==============================] - 61s 667ms/step - loss: 0.1573 - accuracy: 0.9425 - val_loss: 3.5150 - val_accuracy: 0.6175 - lr: 0.0010\n",
            "Epoch 158/300\n",
            "92/92 [==============================] - 58s 626ms/step - loss: 0.1438 - accuracy: 0.9466 - val_loss: 5.9968 - val_accuracy: 0.4454 - lr: 0.0010\n",
            "Epoch 159/300\n",
            "92/92 [==============================] - 55s 593ms/step - loss: 0.1575 - accuracy: 0.9425 - val_loss: 6.6260 - val_accuracy: 0.5929 - lr: 0.0010\n",
            "Epoch 160/300\n",
            "92/92 [==============================] - 57s 622ms/step - loss: 0.1438 - accuracy: 0.9473 - val_loss: 9.1906 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 161/300\n",
            "92/92 [==============================] - 64s 700ms/step - loss: 0.1656 - accuracy: 0.9330 - val_loss: 1.3795 - val_accuracy: 0.6311 - lr: 0.0010\n",
            "Epoch 162/300\n",
            "92/92 [==============================] - 61s 660ms/step - loss: 0.1555 - accuracy: 0.9432 - val_loss: 14.3071 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 163/300\n",
            "92/92 [==============================] - 55s 594ms/step - loss: 0.1608 - accuracy: 0.9405 - val_loss: 4.3959 - val_accuracy: 0.5929 - lr: 0.0010\n",
            "Epoch 164/300\n",
            "92/92 [==============================] - 53s 575ms/step - loss: 0.1519 - accuracy: 0.9432 - val_loss: 2.7715 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 165/300\n",
            "92/92 [==============================] - 53s 577ms/step - loss: 0.1576 - accuracy: 0.9412 - val_loss: 4.3080 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 166/300\n",
            "92/92 [==============================] - 53s 581ms/step - loss: 0.1452 - accuracy: 0.9508 - val_loss: 8.2495 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 167/300\n",
            "92/92 [==============================] - 60s 648ms/step - loss: 0.1564 - accuracy: 0.9412 - val_loss: 0.8719 - val_accuracy: 0.6858 - lr: 0.0010\n",
            "Epoch 168/300\n",
            "92/92 [==============================] - 59s 647ms/step - loss: 0.1650 - accuracy: 0.9405 - val_loss: 12.8664 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 169/300\n",
            "92/92 [==============================] - 57s 621ms/step - loss: 0.1537 - accuracy: 0.9343 - val_loss: 7.7578 - val_accuracy: 0.4344 - lr: 0.0010\n",
            "Epoch 170/300\n",
            "92/92 [==============================] - 69s 746ms/step - loss: 0.1473 - accuracy: 0.9432 - val_loss: 20.7136 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 171/300\n",
            "92/92 [==============================] - 69s 743ms/step - loss: 0.1591 - accuracy: 0.9391 - val_loss: 9.5728 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 172/300\n",
            "92/92 [==============================] - 62s 672ms/step - loss: 0.1631 - accuracy: 0.9378 - val_loss: 7.2510 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 173/300\n",
            "92/92 [==============================] - 56s 604ms/step - loss: 0.1248 - accuracy: 0.9569 - val_loss: 13.5823 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 174/300\n",
            "92/92 [==============================] - 53s 578ms/step - loss: 0.1724 - accuracy: 0.9268 - val_loss: 1.1387 - val_accuracy: 0.6093 - lr: 0.0010\n",
            "Epoch 175/300\n",
            "92/92 [==============================] - 53s 576ms/step - loss: 0.1350 - accuracy: 0.9425 - val_loss: 2.4269 - val_accuracy: 0.5929 - lr: 0.0010\n",
            "Epoch 176/300\n",
            "92/92 [==============================] - 61s 663ms/step - loss: 0.1682 - accuracy: 0.9309 - val_loss: 1.9365 - val_accuracy: 0.6475 - lr: 0.0010\n",
            "Epoch 177/300\n",
            "92/92 [==============================] - 57s 619ms/step - loss: 0.1484 - accuracy: 0.9439 - val_loss: 18.9020 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 178/300\n",
            "92/92 [==============================] - 54s 581ms/step - loss: 0.1388 - accuracy: 0.9460 - val_loss: 2.3081 - val_accuracy: 0.4645 - lr: 0.0010\n",
            "Epoch 179/300\n",
            "92/92 [==============================] - 53s 580ms/step - loss: 0.1449 - accuracy: 0.9432 - val_loss: 16.0696 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 180/300\n",
            "92/92 [==============================] - 53s 575ms/step - loss: 0.1510 - accuracy: 0.9425 - val_loss: 2.3387 - val_accuracy: 0.6448 - lr: 0.0010\n",
            "Epoch 181/300\n",
            "92/92 [==============================] - 53s 577ms/step - loss: 0.1595 - accuracy: 0.9350 - val_loss: 16.9079 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 182/300\n",
            "92/92 [==============================] - 71s 774ms/step - loss: 0.1522 - accuracy: 0.9439 - val_loss: 7.2771 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 183/300\n",
            "92/92 [==============================] - 62s 673ms/step - loss: 0.1523 - accuracy: 0.9432 - val_loss: 3.3494 - val_accuracy: 0.5984 - lr: 0.0010\n",
            "Epoch 184/300\n",
            "92/92 [==============================] - 58s 630ms/step - loss: 0.1531 - accuracy: 0.9371 - val_loss: 0.6212 - val_accuracy: 0.7541 - lr: 0.0010\n",
            "Epoch 185/300\n",
            "92/92 [==============================] - 54s 591ms/step - loss: 0.1521 - accuracy: 0.9460 - val_loss: 0.8848 - val_accuracy: 0.7650 - lr: 0.0010\n",
            "Epoch 186/300\n",
            "92/92 [==============================] - 53s 579ms/step - loss: 0.1317 - accuracy: 0.9466 - val_loss: 1.6970 - val_accuracy: 0.5984 - lr: 0.0010\n",
            "Epoch 187/300\n",
            "92/92 [==============================] - 64s 694ms/step - loss: 0.1562 - accuracy: 0.9364 - val_loss: 15.8437 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 188/300\n",
            "92/92 [==============================] - 60s 657ms/step - loss: 0.1404 - accuracy: 0.9494 - val_loss: 12.0018 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 189/300\n",
            "92/92 [==============================] - 60s 655ms/step - loss: 0.1663 - accuracy: 0.9378 - val_loss: 5.9436 - val_accuracy: 0.3825 - lr: 0.0010\n",
            "Epoch 190/300\n",
            "92/92 [==============================] - 60s 646ms/step - loss: 0.1397 - accuracy: 0.9460 - val_loss: 9.6878 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 191/300\n",
            "92/92 [==============================] - 55s 596ms/step - loss: 0.1342 - accuracy: 0.9590 - val_loss: 9.9809 - val_accuracy: 0.4617 - lr: 0.0010\n",
            "Epoch 192/300\n",
            "92/92 [==============================] - 61s 661ms/step - loss: 0.1311 - accuracy: 0.9501 - val_loss: 1.6793 - val_accuracy: 0.7869 - lr: 0.0010\n",
            "Epoch 193/300\n",
            "92/92 [==============================] - 62s 676ms/step - loss: 0.1316 - accuracy: 0.9487 - val_loss: 2.1959 - val_accuracy: 0.6311 - lr: 0.0010\n",
            "Epoch 194/300\n",
            "92/92 [==============================] - 56s 605ms/step - loss: 0.1275 - accuracy: 0.9535 - val_loss: 10.4872 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 195/300\n",
            "92/92 [==============================] - 53s 572ms/step - loss: 0.1336 - accuracy: 0.9549 - val_loss: 8.7862 - val_accuracy: 0.5929 - lr: 0.0010\n",
            "Epoch 196/300\n",
            "92/92 [==============================] - 53s 574ms/step - loss: 0.1502 - accuracy: 0.9391 - val_loss: 1.8245 - val_accuracy: 0.5929 - lr: 0.0010\n",
            "Epoch 197/300\n",
            "92/92 [==============================] - 53s 579ms/step - loss: 0.1478 - accuracy: 0.9425 - val_loss: 3.2418 - val_accuracy: 0.5929 - lr: 0.0010\n",
            "Epoch 198/300\n",
            "92/92 [==============================] - 59s 645ms/step - loss: 0.1346 - accuracy: 0.9549 - val_loss: 13.8516 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 199/300\n",
            "92/92 [==============================] - 69s 747ms/step - loss: 0.1347 - accuracy: 0.9501 - val_loss: 2.9382 - val_accuracy: 0.5984 - lr: 0.0010\n",
            "Epoch 200/300\n",
            "92/92 [==============================] - 60s 651ms/step - loss: 0.1375 - accuracy: 0.9508 - val_loss: 20.7937 - val_accuracy: 0.3852 - lr: 0.0010\n",
            "Epoch 201/300\n",
            "92/92 [==============================] - 59s 640ms/step - loss: 0.1404 - accuracy: 0.9419 - val_loss: 2.0118 - val_accuracy: 0.6967 - lr: 0.0010\n",
            "Epoch 202/300\n",
            "92/92 [==============================] - 57s 615ms/step - loss: 0.1518 - accuracy: 0.9384 - val_loss: 34.3894 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 203/300\n",
            "92/92 [==============================] - 55s 602ms/step - loss: 0.1344 - accuracy: 0.9466 - val_loss: 2.7705 - val_accuracy: 0.5929 - lr: 0.0010\n",
            "Epoch 204/300\n",
            "92/92 [==============================] - 69s 750ms/step - loss: 0.1199 - accuracy: 0.9617 - val_loss: 1.8957 - val_accuracy: 0.5956 - lr: 0.0010\n",
            "Epoch 205/300\n",
            "92/92 [==============================] - 59s 645ms/step - loss: 0.1193 - accuracy: 0.9569 - val_loss: 21.7806 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 206/300\n",
            "92/92 [==============================] - 70s 762ms/step - loss: 0.1339 - accuracy: 0.9555 - val_loss: 8.5628 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 207/300\n",
            "92/92 [==============================] - 71s 768ms/step - loss: 0.1425 - accuracy: 0.9453 - val_loss: 3.8370 - val_accuracy: 0.5929 - lr: 0.0010\n",
            "Epoch 208/300\n",
            "92/92 [==============================] - 52s 563ms/step - loss: 0.1167 - accuracy: 0.9583 - val_loss: 14.3791 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 209/300\n",
            "92/92 [==============================] - 54s 587ms/step - loss: 0.1398 - accuracy: 0.9466 - val_loss: 1.6954 - val_accuracy: 0.6393 - lr: 0.0010\n",
            "Epoch 210/300\n",
            "92/92 [==============================] - 52s 564ms/step - loss: 0.1317 - accuracy: 0.9453 - val_loss: 15.3383 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 211/300\n",
            "92/92 [==============================] - 52s 569ms/step - loss: 0.1476 - accuracy: 0.9466 - val_loss: 5.0071 - val_accuracy: 0.5956 - lr: 0.0010\n",
            "Epoch 212/300\n",
            "92/92 [==============================] - 54s 586ms/step - loss: 0.1367 - accuracy: 0.9466 - val_loss: 7.3338 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 213/300\n",
            "92/92 [==============================] - 57s 623ms/step - loss: 0.1308 - accuracy: 0.9521 - val_loss: 23.6575 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 214/300\n",
            "92/92 [==============================] - 52s 568ms/step - loss: 0.1316 - accuracy: 0.9542 - val_loss: 21.5308 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 215/300\n",
            "92/92 [==============================] - 52s 565ms/step - loss: 0.1284 - accuracy: 0.9487 - val_loss: 10.0515 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 216/300\n",
            "92/92 [==============================] - 61s 665ms/step - loss: 0.1123 - accuracy: 0.9590 - val_loss: 9.2511 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 217/300\n",
            "92/92 [==============================] - 65s 706ms/step - loss: 0.1233 - accuracy: 0.9569 - val_loss: 18.7603 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 218/300\n",
            "92/92 [==============================] - 60s 655ms/step - loss: 0.1330 - accuracy: 0.9562 - val_loss: 11.6309 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 219/300\n",
            "92/92 [==============================] - 52s 569ms/step - loss: 0.1413 - accuracy: 0.9494 - val_loss: 7.1979 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 220/300\n",
            "92/92 [==============================] - 53s 573ms/step - loss: 0.1410 - accuracy: 0.9425 - val_loss: 30.4215 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 221/300\n",
            "92/92 [==============================] - 52s 559ms/step - loss: 0.1585 - accuracy: 0.9425 - val_loss: 3.5005 - val_accuracy: 0.5055 - lr: 0.0010\n",
            "Epoch 222/300\n",
            "92/92 [==============================] - 52s 565ms/step - loss: 0.1387 - accuracy: 0.9480 - val_loss: 6.6866 - val_accuracy: 0.5546 - lr: 0.0010\n",
            "Epoch 223/300\n",
            "92/92 [==============================] - 54s 589ms/step - loss: 0.1177 - accuracy: 0.9576 - val_loss: 2.6452 - val_accuracy: 0.6612 - lr: 0.0010\n",
            "Epoch 224/300\n",
            "92/92 [==============================] - 56s 607ms/step - loss: 0.1359 - accuracy: 0.9480 - val_loss: 2.5559 - val_accuracy: 0.6284 - lr: 0.0010\n",
            "Epoch 225/300\n",
            "92/92 [==============================] - 52s 564ms/step - loss: 0.1287 - accuracy: 0.9487 - val_loss: 4.4619 - val_accuracy: 0.6120 - lr: 0.0010\n",
            "Epoch 226/300\n",
            "92/92 [==============================] - 52s 569ms/step - loss: 0.1249 - accuracy: 0.9590 - val_loss: 3.1257 - val_accuracy: 0.6175 - lr: 0.0010\n",
            "Epoch 227/300\n",
            "92/92 [==============================] - 52s 566ms/step - loss: 0.1436 - accuracy: 0.9446 - val_loss: 5.4661 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 228/300\n",
            "92/92 [==============================] - 59s 637ms/step - loss: 0.1325 - accuracy: 0.9535 - val_loss: 7.3408 - val_accuracy: 0.5984 - lr: 0.0010\n",
            "Epoch 229/300\n",
            "92/92 [==============================] - 65s 710ms/step - loss: 0.1285 - accuracy: 0.9501 - val_loss: 24.2530 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 230/300\n",
            "92/92 [==============================] - 76s 825ms/step - loss: 0.1558 - accuracy: 0.9432 - val_loss: 11.3127 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 231/300\n",
            "92/92 [==============================] - 63s 678ms/step - loss: 0.1121 - accuracy: 0.9610 - val_loss: 11.2557 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 232/300\n",
            "92/92 [==============================] - 60s 650ms/step - loss: 0.1171 - accuracy: 0.9542 - val_loss: 5.3125 - val_accuracy: 0.4699 - lr: 0.0010\n",
            "Epoch 233/300\n",
            "92/92 [==============================] - 54s 586ms/step - loss: 0.1084 - accuracy: 0.9624 - val_loss: 9.4577 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 234/300\n",
            "92/92 [==============================] - 56s 611ms/step - loss: 0.1206 - accuracy: 0.9583 - val_loss: 3.5292 - val_accuracy: 0.5984 - lr: 0.0010\n",
            "Epoch 235/300\n",
            "92/92 [==============================] - 52s 562ms/step - loss: 0.1198 - accuracy: 0.9569 - val_loss: 7.6929 - val_accuracy: 0.4672 - lr: 0.0010\n",
            "Epoch 236/300\n",
            "92/92 [==============================] - 52s 565ms/step - loss: 0.1331 - accuracy: 0.9487 - val_loss: 9.0230 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 237/300\n",
            "92/92 [==============================] - 52s 562ms/step - loss: 0.1432 - accuracy: 0.9425 - val_loss: 0.4705 - val_accuracy: 0.8497 - lr: 0.0010\n",
            "Epoch 238/300\n",
            "92/92 [==============================] - 52s 560ms/step - loss: 0.1208 - accuracy: 0.9549 - val_loss: 3.1361 - val_accuracy: 0.6749 - lr: 0.0010\n",
            "Epoch 239/300\n",
            "92/92 [==============================] - 52s 563ms/step - loss: 0.1252 - accuracy: 0.9508 - val_loss: 1.8121 - val_accuracy: 0.6393 - lr: 0.0010\n",
            "Epoch 240/300\n",
            "92/92 [==============================] - 52s 560ms/step - loss: 0.1319 - accuracy: 0.9494 - val_loss: 8.9880 - val_accuracy: 0.4399 - lr: 0.0010\n",
            "Epoch 241/300\n",
            "92/92 [==============================] - 51s 558ms/step - loss: 0.1190 - accuracy: 0.9549 - val_loss: 8.7132 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 242/300\n",
            "92/92 [==============================] - 52s 562ms/step - loss: 0.1508 - accuracy: 0.9494 - val_loss: 14.6466 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 243/300\n",
            "92/92 [==============================] - 51s 558ms/step - loss: 0.1292 - accuracy: 0.9494 - val_loss: 10.7472 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 244/300\n",
            "92/92 [==============================] - 51s 556ms/step - loss: 0.1299 - accuracy: 0.9535 - val_loss: 23.6877 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 245/300\n",
            "92/92 [==============================] - 56s 609ms/step - loss: 0.1197 - accuracy: 0.9562 - val_loss: 35.3703 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 246/300\n",
            "92/92 [==============================] - 54s 589ms/step - loss: 0.1217 - accuracy: 0.9555 - val_loss: 16.4438 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 247/300\n",
            "92/92 [==============================] - 61s 652ms/step - loss: 0.1387 - accuracy: 0.9425 - val_loss: 4.8665 - val_accuracy: 0.5956 - lr: 0.0010\n",
            "Epoch 248/300\n",
            "92/92 [==============================] - 56s 607ms/step - loss: 0.1093 - accuracy: 0.9590 - val_loss: 5.7773 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 249/300\n",
            "92/92 [==============================] - 52s 566ms/step - loss: 0.1217 - accuracy: 0.9549 - val_loss: 9.0461 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 250/300\n",
            "92/92 [==============================] - 51s 558ms/step - loss: 0.1354 - accuracy: 0.9494 - val_loss: 5.7140 - val_accuracy: 0.5956 - lr: 0.0010\n",
            "Epoch 251/300\n",
            "92/92 [==============================] - 56s 606ms/step - loss: 0.1206 - accuracy: 0.9590 - val_loss: 1.1335 - val_accuracy: 0.5929 - lr: 0.0010\n",
            "Epoch 252/300\n",
            "92/92 [==============================] - 57s 619ms/step - loss: 0.1103 - accuracy: 0.9555 - val_loss: 0.6062 - val_accuracy: 0.8115 - lr: 0.0010\n",
            "Epoch 253/300\n",
            "92/92 [==============================] - 71s 770ms/step - loss: 0.1295 - accuracy: 0.9487 - val_loss: 23.1539 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 254/300\n",
            "92/92 [==============================] - 63s 685ms/step - loss: 0.1256 - accuracy: 0.9542 - val_loss: 55.1248 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 255/300\n",
            "92/92 [==============================] - 63s 684ms/step - loss: 0.1312 - accuracy: 0.9521 - val_loss: 19.4408 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 256/300\n",
            "92/92 [==============================] - 59s 645ms/step - loss: 0.1316 - accuracy: 0.9446 - val_loss: 16.3113 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 257/300\n",
            "92/92 [==============================] - 63s 683ms/step - loss: 0.1374 - accuracy: 0.9466 - val_loss: 35.1798 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 258/300\n",
            "92/92 [==============================] - 56s 612ms/step - loss: 0.1140 - accuracy: 0.9583 - val_loss: 9.8005 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 259/300\n",
            "92/92 [==============================] - 52s 561ms/step - loss: 0.1073 - accuracy: 0.9631 - val_loss: 15.4958 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 260/300\n",
            "92/92 [==============================] - 52s 562ms/step - loss: 0.1262 - accuracy: 0.9453 - val_loss: 11.8213 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 261/300\n",
            "92/92 [==============================] - 52s 560ms/step - loss: 0.1286 - accuracy: 0.9508 - val_loss: 11.0156 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 262/300\n",
            "92/92 [==============================] - 52s 570ms/step - loss: 0.1252 - accuracy: 0.9542 - val_loss: 11.7986 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 263/300\n",
            "92/92 [==============================] - 51s 559ms/step - loss: 0.1023 - accuracy: 0.9603 - val_loss: 17.8598 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 264/300\n",
            "92/92 [==============================] - 60s 652ms/step - loss: 0.1204 - accuracy: 0.9508 - val_loss: 7.0336 - val_accuracy: 0.4235 - lr: 0.0010\n",
            "Epoch 265/300\n",
            "92/92 [==============================] - 61s 662ms/step - loss: 0.1253 - accuracy: 0.9514 - val_loss: 12.1546 - val_accuracy: 0.4317 - lr: 0.0010\n",
            "Epoch 266/300\n",
            "92/92 [==============================] - 69s 752ms/step - loss: 0.1127 - accuracy: 0.9610 - val_loss: 4.1869 - val_accuracy: 0.6967 - lr: 0.0010\n",
            "Epoch 267/300\n",
            "92/92 [==============================] - 64s 691ms/step - loss: 0.1238 - accuracy: 0.9583 - val_loss: 8.4117 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 268/300\n",
            "92/92 [==============================] - 55s 591ms/step - loss: 0.1096 - accuracy: 0.9624 - val_loss: 16.3613 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 269/300\n",
            "92/92 [==============================] - 51s 560ms/step - loss: 0.1153 - accuracy: 0.9555 - val_loss: 10.5505 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 270/300\n",
            "92/92 [==============================] - 52s 565ms/step - loss: 0.1161 - accuracy: 0.9555 - val_loss: 34.9460 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 271/300\n",
            "92/92 [==============================] - 52s 565ms/step - loss: 0.1168 - accuracy: 0.9521 - val_loss: 10.1974 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 272/300\n",
            "92/92 [==============================] - 56s 611ms/step - loss: 0.1327 - accuracy: 0.9446 - val_loss: 4.8597 - val_accuracy: 0.4836 - lr: 0.0010\n",
            "Epoch 273/300\n",
            "92/92 [==============================] - 62s 677ms/step - loss: 0.1197 - accuracy: 0.9549 - val_loss: 37.6431 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 274/300\n",
            "92/92 [==============================] - 63s 680ms/step - loss: 0.1249 - accuracy: 0.9521 - val_loss: 2.1582 - val_accuracy: 0.6038 - lr: 0.0010\n",
            "Epoch 275/300\n",
            "92/92 [==============================] - 62s 677ms/step - loss: 0.1085 - accuracy: 0.9617 - val_loss: 2.3972 - val_accuracy: 0.6612 - lr: 0.0010\n",
            "Epoch 276/300\n",
            "92/92 [==============================] - 67s 724ms/step - loss: 0.1088 - accuracy: 0.9651 - val_loss: 22.6167 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 277/300\n",
            "92/92 [==============================] - 69s 751ms/step - loss: 0.1233 - accuracy: 0.9460 - val_loss: 2.7949 - val_accuracy: 0.5519 - lr: 0.0010\n",
            "Epoch 278/300\n",
            "92/92 [==============================] - 63s 683ms/step - loss: 0.1190 - accuracy: 0.9535 - val_loss: 12.9876 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 279/300\n",
            "92/92 [==============================] - 57s 617ms/step - loss: 0.1117 - accuracy: 0.9555 - val_loss: 14.2871 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 280/300\n",
            "92/92 [==============================] - 53s 576ms/step - loss: 0.1251 - accuracy: 0.9528 - val_loss: 3.8227 - val_accuracy: 0.6093 - lr: 0.0010\n",
            "Epoch 281/300\n",
            "92/92 [==============================] - 53s 579ms/step - loss: 0.1170 - accuracy: 0.9501 - val_loss: 3.6837 - val_accuracy: 0.6230 - lr: 0.0010\n",
            "Epoch 282/300\n",
            "92/92 [==============================] - 52s 570ms/step - loss: 0.1317 - accuracy: 0.9521 - val_loss: 9.4211 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 283/300\n",
            "92/92 [==============================] - 52s 561ms/step - loss: 0.1040 - accuracy: 0.9596 - val_loss: 2.4290 - val_accuracy: 0.5492 - lr: 0.0010\n",
            "Epoch 284/300\n",
            "92/92 [==============================] - 52s 565ms/step - loss: 0.1217 - accuracy: 0.9535 - val_loss: 6.2230 - val_accuracy: 0.5984 - lr: 0.0010\n",
            "Epoch 285/300\n",
            "92/92 [==============================] - 55s 599ms/step - loss: 0.1180 - accuracy: 0.9555 - val_loss: 2.7217 - val_accuracy: 0.5492 - lr: 0.0010\n",
            "Epoch 286/300\n",
            "92/92 [==============================] - 63s 688ms/step - loss: 0.1306 - accuracy: 0.9508 - val_loss: 10.0579 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 287/300\n",
            "92/92 [==============================] - 73s 792ms/step - loss: 0.1263 - accuracy: 0.9501 - val_loss: 8.9728 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 288/300\n",
            "92/92 [==============================] - 63s 677ms/step - loss: 0.0903 - accuracy: 0.9651 - val_loss: 8.3598 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 289/300\n",
            "92/92 [==============================] - 57s 617ms/step - loss: 0.1162 - accuracy: 0.9624 - val_loss: 5.7927 - val_accuracy: 0.5956 - lr: 0.0010\n",
            "Epoch 290/300\n",
            "92/92 [==============================] - 56s 606ms/step - loss: 0.1093 - accuracy: 0.9610 - val_loss: 7.2086 - val_accuracy: 0.5956 - lr: 0.0010\n",
            "Epoch 291/300\n",
            "92/92 [==============================] - 63s 687ms/step - loss: 0.1274 - accuracy: 0.9487 - val_loss: 2.3051 - val_accuracy: 0.6175 - lr: 0.0010\n",
            "Epoch 292/300\n",
            "92/92 [==============================] - 63s 685ms/step - loss: 0.1111 - accuracy: 0.9569 - val_loss: 4.5128 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 293/300\n",
            "92/92 [==============================] - 60s 649ms/step - loss: 0.1143 - accuracy: 0.9583 - val_loss: 7.4388 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 294/300\n",
            "92/92 [==============================] - 51s 555ms/step - loss: 0.1299 - accuracy: 0.9514 - val_loss: 2.2825 - val_accuracy: 0.6093 - lr: 0.0010\n",
            "Epoch 295/300\n",
            "92/92 [==============================] - 53s 579ms/step - loss: 0.1029 - accuracy: 0.9637 - val_loss: 2.7016 - val_accuracy: 0.6612 - lr: 0.0010\n",
            "Epoch 296/300\n",
            "92/92 [==============================] - 65s 703ms/step - loss: 0.1290 - accuracy: 0.9535 - val_loss: 29.0702 - val_accuracy: 0.5874 - lr: 0.0010\n",
            "Epoch 297/300\n",
            "92/92 [==============================] - 62s 668ms/step - loss: 0.1082 - accuracy: 0.9617 - val_loss: 6.7253 - val_accuracy: 0.5956 - lr: 0.0010\n",
            "Epoch 298/300\n",
            "92/92 [==============================] - 54s 584ms/step - loss: 0.1041 - accuracy: 0.9617 - val_loss: 5.2815 - val_accuracy: 0.4781 - lr: 0.0010\n",
            "Epoch 299/300\n",
            "92/92 [==============================] - 63s 680ms/step - loss: 0.1115 - accuracy: 0.9596 - val_loss: 1.3113 - val_accuracy: 0.6284 - lr: 0.0010\n",
            "Epoch 300/300\n",
            "92/92 [==============================] - 57s 622ms/step - loss: 0.1006 - accuracy: 0.9651 - val_loss: 2.8982 - val_accuracy: 0.6284 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "history_lstm = cnn.fit(X_train_cnn, y_train_cnn, epochs=300, batch_size=mini_batch_size, callbacks=callbacks,\n",
        "                       validation_data=(X_val_cnn, y_val_cnn)).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 99s 5s/step\n",
            "Accuracy 0.46153846153846156\n",
            "F1-score [0.53972603 0.35135135]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.75      0.54       264\n",
            "           1       0.58      0.25      0.35       360\n",
            "\n",
            "    accuracy                           0.46       624\n",
            "   macro avg       0.50      0.50      0.45       624\n",
            "weighted avg       0.51      0.46      0.43       624\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = np.argmax(lstm.predict(X_test_cnn), axis=1)\n",
        "\n",
        "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 118s 6s/step - loss: 0.6931 - accuracy: 0.4679\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.6931281685829163, 0.46794870495796204]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm.evaluate(X_test_cnn, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "#cerco di vedere se posso migliorare questo LSTM \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizza layer LSTM stratificati: Puoi provare ad aggiungere più layer LSTM uno sopra l'altro per catturare dipendenze a diverse scale temporali. Ad esempio, puoi aggiungere un secondo layer LSTM dopo il primo:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Strati LSTM stratificati: Ho aggiunto un secondo strato LSTM con 128 unità dopo il primo strato LSTM. Il secondo strato LSTM è seguito da un Dropout per ridurre l'overfitting.\n",
        "\n",
        "Livello di GlobalMaxPooling1D: Ho aggiunto un livello di GlobalMaxPooling1D dopo gli strati LSTM per ridurre la dimensione delle feature map senza perdere completamente l'informazione temporale.\n",
        "\n",
        "Livello Fully Connected più profondo: Ho aumentato la profondità del livello Fully Connected aggiungendo un ulteriore livello Dense con 128 unità.\n",
        "\n",
        "Regolarizzazione più intensa: Ho aumentato l'intensità del Dropout a 0.5 per ridurre l'overfitting.\n",
        "\n",
        "Ottimizzatore Adam con learning rate adattativo: Ho specificato un ottimizzatore Adam con un learning rate di 0.001 per l'addestramento del modello."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense, GlobalMaxPooling1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_modified_lstm(n_timesteps, n_outputs):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(256, input_shape=(n_timesteps, 1), return_sequences=True))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
        "    \n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    \n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "lstm1 = build_modified_lstm(n_timesteps, n_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "92/92 [==============================] - 58s 630ms/step - loss: 0.4109 - accuracy: 0.8228 - val_loss: 0.8282 - val_accuracy: 0.6011 - lr: 0.0010\n",
            "Epoch 2/7\n",
            "92/92 [==============================] - 56s 606ms/step - loss: 0.3961 - accuracy: 0.8372 - val_loss: 8.4958 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 3/7\n",
            "92/92 [==============================] - 53s 572ms/step - loss: 0.3917 - accuracy: 0.8263 - val_loss: 0.6182 - val_accuracy: 0.6639 - lr: 0.0010\n",
            "Epoch 4/7\n",
            "92/92 [==============================] - 66s 720ms/step - loss: 0.3914 - accuracy: 0.8386 - val_loss: 0.8939 - val_accuracy: 0.5902 - lr: 0.0010\n",
            "Epoch 5/7\n",
            "92/92 [==============================] - 61s 668ms/step - loss: 0.3870 - accuracy: 0.8345 - val_loss: 0.7579 - val_accuracy: 0.4809 - lr: 0.0010\n",
            "Epoch 6/7\n",
            "92/92 [==============================] - 62s 673ms/step - loss: 0.3673 - accuracy: 0.8543 - val_loss: 0.8533 - val_accuracy: 0.4590 - lr: 0.0010\n",
            "Epoch 7/7\n",
            "92/92 [==============================] - 64s 697ms/step - loss: 0.3751 - accuracy: 0.8331 - val_loss: 0.5443 - val_accuracy: 0.7377 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "history_lstm = cnn.fit(X_train_cnn, y_train_cnn, epochs=7, batch_size=mini_batch_size, callbacks=callbacks,\n",
        "                       validation_data=(X_val_cnn, y_val_cnn)).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 173s 9s/step\n",
            "Accuracy 0.5689102564102564\n",
            "F1-score [0.         0.72522983]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       264\n",
            "           1       0.57      0.99      0.73       360\n",
            "\n",
            "    accuracy                           0.57       624\n",
            "   macro avg       0.29      0.49      0.36       624\n",
            "weighted avg       0.33      0.57      0.42       624\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = np.argmax(lstm.predict(X_test_cnn), axis=1)\n",
        "\n",
        "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# shapelet "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All the packages were installed\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install tslearn\n",
        "!pip install pyts\n",
        "!pip install kds\n",
        "!pip install matrixprofile-ts\n",
        "!pip install sktime\n",
        "!pip install prefixspan\n",
        "!pip install spmf\n",
        "\n",
        "clear_output(wait=True)\n",
        "print('All the packages were installed')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Time Series Preprocessing\n",
        "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "\n",
        "#Time Series Approximation\n",
        "from pyts.approximation import DiscreteFourierTransform\n",
        "from tslearn.piecewise import PiecewiseAggregateApproximation\n",
        "from tslearn.piecewise import SymbolicAggregateApproximation\n",
        "\n",
        "#Main classification metrics and utilities\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import kds #for the lift\n",
        "\n",
        "#Classifiers\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from pyts.classification import KNeighborsClassifier\n",
        "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
        "\n",
        "#Time Series Clustering\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "from matplotlib.pyplot import cm\n",
        "\n",
        "#Shapelets\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "from tslearn.shapelets import LearningShapelets\n",
        "from tslearn.shapelets import grabocka_params_to_shapelet_size_dict\n",
        "from pyts.transformation import ShapeletTransform\n",
        "\n",
        "#matrixprofile\n",
        "from matrixprofile import motifs\n",
        "from matrixprofile.discords import discords\n",
        "from matrixprofile import matrixProfile\n",
        "\n",
        "#CNN\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling1D\n",
        "from keras.layers import Conv1D, Activation, Conv1D, BatchNormalization\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "#Rocket\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sktime.transformations.panel.rocket import MiniRocket\n",
        "\n",
        "#Sequential Pattern Mining\n",
        "from prefixspan import PrefixSpan\n",
        "from spmf import Spmf\n",
        "\n",
        "\n",
        "#To avoid repetitive warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "Xv = X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_ts 1828\n",
            "ts_sz 7500\n",
            "n_classes 2\n",
            "shapelet_sizes {750: 7}\n"
          ]
        }
      ],
      "source": [
        "n_ts, ts_sz = Xv.shape\n",
        "n_classes = len(set(y))\n",
        "\n",
        "# Set the number of shapelets per size as done in the original paper\n",
        "shapelet_sizes = grabocka_params_to_shapelet_size_dict(n_ts=n_ts,\n",
        "                                                       ts_sz=ts_sz,\n",
        "                                                       n_classes=n_classes,\n",
        "                                                       l=0.1,\n",
        "                                                       r=1)\n",
        "\n",
        "print('n_ts', n_ts)\n",
        "print('ts_sz', ts_sz)\n",
        "print('n_classes', n_classes)\n",
        "print('shapelet_sizes', shapelet_sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.9/site-packages/tslearn/shapelets/shapelets.py:354: FutureWarning: The default value for 'scale' is set to False in version 0.4 to ensure backward compatibility, but is likely to change in a future version.\n",
            "  warnings.warn(\"The default value for 'scale' is set to False \"\n"
          ]
        }
      ],
      "source": [
        "# Define the model using parameters provided by the authors (except that we use\n",
        "# fewer iterations here)\n",
        "shp_clf = LearningShapelets(n_shapelets_per_size=shapelet_sizes,\n",
        "                        optimizer=\"sgd\",\n",
        "            \n",
        "                        weight_regularizer=.01,\n",
        "                        max_iter=1,\n",
        "                        verbose=1, scale=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mNon è possibile eseguire il codice. La sessione è stata eliminata. Provare a riavviare il kernel."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mSi è verificato un arresto anomalo del kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. Esaminare il codice nelle celle per identificare una possibile causa dell'errore. Per altre informazioni, fare clic su <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a>. Per altri dettagli, vedere Jupyter <a href='command:jupyter.viewOutput'>log</a>."
          ]
        }
      ],
      "source": [
        "shp_clf.fit(Xv, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_labels = shp_clf.predict(Xv)\n",
        "print(\"Correct classification rate:\", accuracy_score(y, predicted_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_locations = shp_clf.locate(Xv)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
